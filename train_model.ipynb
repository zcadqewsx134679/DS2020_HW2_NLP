{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "category = []\n",
    "label = []\n",
    "train_category=[]\n",
    "for i in range(df.shape[0]):\n",
    "    category.append(df['Category'][i])\n",
    "    label.append(df['Label'][i])\n",
    "tokenizer = Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(category)\n",
    "category_index = tokenizer.word_index\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    train_category.append(category_index[df['Category'][i]])\n",
    "\n",
    "label = np.array(label)\n",
    "train_category = to_categorical(np.asarray(train_category))\n",
    "'''\n",
    "category = []\n",
    "label = []\n",
    "label_test = []\n",
    "train_category=[]\n",
    "test_category=[]\n",
    "for i in range(df.shape[0]):\n",
    "    category.append(df['Category'][i])\n",
    "    label.append(df['Label'][i])\n",
    "for j in range(df2.shape[0]):\n",
    "    label_test.append(df2['Label'][j])\n",
    "tokenizer = Tokenizer(nb_words=None)\n",
    "tokenizer.fit_on_texts(category)\n",
    "category_index = tokenizer.word_index\n",
    "category_index.setdefault('living',23)\n",
    "category_index.setdefault('middleeast',24)\n",
    "category_index.setdefault('us',25)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    train_category.append(category_index[df['Category'][i]])\n",
    "for j in range(df2.shape[0]):\n",
    "    test_category.append(category_index[df2['Category'][j]])\n",
    "label = np.array(label)\n",
    "label_test = np.array(label_test)\n",
    "label-=3\n",
    "train_category = to_categorical(np.asarray(train_category))\n",
    "padding = np.zeros((2040,3))\n",
    "train_category = np.concatenate((train_category,padding),axis=1)\n",
    "test_category = to_categorical(np.asarray(test_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','r',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "data = []\n",
    "data_test = []\n",
    "for i in range(df.shape[0]):\n",
    "    sent=[(df['Headline'][i])]\n",
    "    stopwords=[\",\",\"the\"]\n",
    "    list=[word for word in word_tokenize(sent[0]) if word not in stopwords]\n",
    "    pos = nltk.pos_tag(list)\n",
    "    wordnet_pos = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, tag in pos:\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos.append(wordnet.ADJ)\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos.append(wordnet.VERB)\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos.append(wordnet.NOUN)\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos.append(wordnet.ADV)\n",
    "        else:\n",
    "            wordnet_pos.append(wordnet.NOUN)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(pos[n][0], pos=wordnet_pos[n]) for n in range(len(pos))]\n",
    "    data.append(tokens)\n",
    "    \n",
    "for j in range(df2.shape[0]):\n",
    "    sent=[(df2['Headline'][j])]\n",
    "    stopwords=[\",\",\"the\"]\n",
    "    list=[word for word in word_tokenize(sent[0]) if word not in stopwords]\n",
    "    pos = nltk.pos_tag(list)\n",
    "    wordnet_pos = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, tag in pos:\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos.append(wordnet.ADJ)\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos.append(wordnet.VERB)\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos.append(wordnet.NOUN)\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos.append(wordnet.ADV)\n",
    "        else:\n",
    "            wordnet_pos.append(wordnet.NOUN)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(pos[n][0], pos=wordnet_pos[n]) for n in range(len(pos))]\n",
    "    data_test.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8753 unique tokens.\n",
      "Found 1692 unique tokens.\n",
      "Found 9225 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_train = Tokenizer(nb_words=None)\n",
    "tokenizer_test = Tokenizer(nb_words=None)\n",
    "tokenizer_train.fit_on_texts(data)\n",
    "tokenizer_test.fit_on_texts(data_test)\n",
    "\n",
    "data_total = data + data_test\n",
    "tokenizer_total = Tokenizer(nb_words=None)\n",
    "tokenizer_total.fit_on_texts(data_total)\n",
    "\n",
    "sequences = tokenizer_train.texts_to_sequences(data)\n",
    "sequences_test = tokenizer_train.texts_to_sequences(data_test)\n",
    "\n",
    "word_index = tokenizer_train.word_index\n",
    "word_index_test = tokenizer_test.word_index\n",
    "word_index_total = tokenizer_total.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Found %s unique tokens.' % len(word_index_test))\n",
    "print('Found %s unique tokens.' % len(word_index_total))\n",
    "\n",
    "data1 = pad_sequences(sequences, maxlen=40)\n",
    "data_test1 = pad_sequences(sequences_test, maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index_total) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index_total) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=40,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_validation_samples = int(0.2 * data1.shape[0])\n",
    "'''\n",
    "x_train1 = data1[:-nb_validation_samples]\n",
    "x_train2 = train_category[:-nb_validation_samples]\n",
    "x_test1 = data_test1\n",
    "x_test2 = test_category\n",
    "y_train = label[:-nb_validation_samples]\n",
    "\n",
    "x_val1 = data1[-nb_validation_samples:]\n",
    "x_val2 = train_category[-nb_validation_samples:]\n",
    "y_val = label[-nb_validation_samples:]\n",
    "'''\n",
    "x_train1 = data1\n",
    "x_train2 = train_category\n",
    "x_test1 = data_test1\n",
    "x_test2 = test_category\n",
    "y_train = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(40,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x1 = Conv1D(filters=100, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\")(embedded_sequences)\n",
    "x1 = Conv1D(filters=200, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\")(x1)\n",
    "x1 = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x1)\n",
    "#x1 = Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\")(x1)\n",
    "#x1 = Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\")(x1)\n",
    "x1 = SpatialDropout1D(0.2)(x1)\n",
    "x1 = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(x1)\n",
    "category_input = Input(shape=(26,))\n",
    "x = concatenate([x1,category_input],axis=-1)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "output = Dense(1)(x)\n",
    "model = Model([sequence_input,category_input],output)\n",
    "model.compile(loss='mse',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 100)      922600      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 40, 100)      50100       embedding_2[11][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 40, 200)      100200      conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 36, 200)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_17 (SpatialDr (None, 36, 200)      0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 128)          168448      spatial_dropout1d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 154)          0           lstm_17[0][0]                    \n",
      "                                                                 input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 154)          616         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 256)          39680       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 256)          0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256)          1024        leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 256)          65792       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 256)          0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 1)            257         leaky_re_lu_34[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,348,717\n",
      "Trainable params: 425,297\n",
      "Non-trainable params: 923,420\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1632 samples, validate on 408 samples\n",
      "Epoch 1/50\n",
      "1632/1632 [==============================] - 7s 4ms/step - loss: 0.9592 - val_loss: 0.4975\n",
      "Epoch 2/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.6943 - val_loss: 0.5368\n",
      "Epoch 3/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.6412 - val_loss: 0.5773\n",
      "Epoch 4/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.6096 - val_loss: 0.5485\n",
      "Epoch 5/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5819 - val_loss: 0.5189\n",
      "Epoch 6/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5889 - val_loss: 0.5557\n",
      "Epoch 7/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5817 - val_loss: 0.5092\n",
      "Epoch 8/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.6010 - val_loss: 0.5756\n",
      "Epoch 9/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5796 - val_loss: 0.5054\n",
      "Epoch 10/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5551 - val_loss: 0.4751\n",
      "Epoch 11/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5597 - val_loss: 0.4932\n",
      "Epoch 12/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5354 - val_loss: 0.6670\n",
      "Epoch 13/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5278 - val_loss: 0.6147\n",
      "Epoch 14/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.5050 - val_loss: 0.5801\n",
      "Epoch 15/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.4914 - val_loss: 0.4920\n",
      "Epoch 16/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.4878 - val_loss: 0.6979\n",
      "Epoch 17/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.4440 - val_loss: 0.5508\n",
      "Epoch 18/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.4278 - val_loss: 0.5005\n",
      "Epoch 19/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.4016 - val_loss: 0.5919\n",
      "Epoch 20/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.3587 - val_loss: 0.5657\n",
      "Epoch 21/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.3026 - val_loss: 0.5948\n",
      "Epoch 22/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.2390 - val_loss: 1.0266\n",
      "Epoch 23/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.1880 - val_loss: 1.0417\n",
      "Epoch 24/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.1378 - val_loss: 0.8503\n",
      "Epoch 25/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.1073 - val_loss: 0.8927\n",
      "Epoch 26/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.1005 - val_loss: 0.7584\n",
      "Epoch 27/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.1052 - val_loss: 0.8463\n",
      "Epoch 28/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0853 - val_loss: 0.8657\n",
      "Epoch 29/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0769 - val_loss: 0.8428\n",
      "Epoch 30/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0705 - val_loss: 0.7073\n",
      "Epoch 31/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0572 - val_loss: 0.6655\n",
      "Epoch 32/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0574 - val_loss: 0.6605\n",
      "Epoch 33/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0460 - val_loss: 0.6365\n",
      "Epoch 34/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0491 - val_loss: 0.7053\n",
      "Epoch 35/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0425 - val_loss: 0.7679\n",
      "Epoch 36/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0436 - val_loss: 0.6009\n",
      "Epoch 37/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0475 - val_loss: 0.8003\n",
      "Epoch 38/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.6750\n",
      "Epoch 39/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0436 - val_loss: 0.6059\n",
      "Epoch 40/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0424 - val_loss: 0.6941\n",
      "Epoch 41/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0357 - val_loss: 0.7147\n",
      "Epoch 42/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0341 - val_loss: 0.7675\n",
      "Epoch 43/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0401 - val_loss: 0.5924\n",
      "Epoch 44/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0392 - val_loss: 0.6338\n",
      "Epoch 45/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0396 - val_loss: 0.6993\n",
      "Epoch 46/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0388 - val_loss: 0.6197\n",
      "Epoch 47/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0415 - val_loss: 0.7568\n",
      "Epoch 48/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0384 - val_loss: 0.6557\n",
      "Epoch 49/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0321 - val_loss: 0.7010\n",
      "Epoch 50/50\n",
      "1632/1632 [==============================] - 5s 3ms/step - loss: 0.0347 - val_loss: 0.6820\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit([x_train1,x_train1,x_train2],y_train,shuffle= True,validation_data=([x_val1,x_val1,x_val2],y_val),epochs=4,batch_size=128)\n",
    "history = model.fit([x_train1,x_train2],y_train,shuffle= True,validation_split=0.2,epochs=10,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predict = model.predict([x_test1,x_test1,x_test2])\n",
    "y_predict = model.predict([x_test1,x_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.607104 ],\n",
       "       [2.4373333],\n",
       "       [1.8779496],\n",
       "       [2.7679307],\n",
       "       [2.5950289],\n",
       "       [2.7514975],\n",
       "       [1.621383 ],\n",
       "       [2.1831155],\n",
       "       [2.8775349],\n",
       "       [2.1246736],\n",
       "       [3.326408 ],\n",
       "       [2.8893075],\n",
       "       [3.0848923],\n",
       "       [2.7587354],\n",
       "       [2.5588312],\n",
       "       [2.9568   ],\n",
       "       [2.8802156],\n",
       "       [2.5639138],\n",
       "       [2.7365332],\n",
       "       [3.1580179],\n",
       "       [2.33274  ],\n",
       "       [3.1200695],\n",
       "       [3.7010674],\n",
       "       [2.9455743],\n",
       "       [2.6477714],\n",
       "       [2.5423355],\n",
       "       [2.6827362],\n",
       "       [2.510148 ],\n",
       "       [2.9709907],\n",
       "       [2.5781837],\n",
       "       [2.312414 ],\n",
       "       [2.665026 ],\n",
       "       [2.8191102],\n",
       "       [1.698108 ],\n",
       "       [2.8154798],\n",
       "       [2.0211797],\n",
       "       [2.830543 ],\n",
       "       [2.7645874],\n",
       "       [2.7129095],\n",
       "       [3.2562976],\n",
       "       [2.882338 ],\n",
       "       [2.2456806],\n",
       "       [3.0294497],\n",
       "       [2.4370682],\n",
       "       [2.7587154],\n",
       "       [2.1143892],\n",
       "       [2.142542 ],\n",
       "       [1.9081217],\n",
       "       [2.7535613],\n",
       "       [2.511846 ],\n",
       "       [2.539366 ],\n",
       "       [3.2904687],\n",
       "       [2.431047 ],\n",
       "       [2.7962613],\n",
       "       [2.6944227],\n",
       "       [2.7942626],\n",
       "       [2.3329668],\n",
       "       [2.9207714],\n",
       "       [2.1036994],\n",
       "       [3.2611961],\n",
       "       [1.9792848],\n",
       "       [2.1634316],\n",
       "       [2.5007482],\n",
       "       [2.931731 ],\n",
       "       [2.2810175],\n",
       "       [2.9980648],\n",
       "       [2.516453 ],\n",
       "       [2.103723 ],\n",
       "       [3.3790894],\n",
       "       [2.8699734],\n",
       "       [2.712138 ],\n",
       "       [2.9024272],\n",
       "       [2.065056 ],\n",
       "       [2.4613981],\n",
       "       [2.547613 ],\n",
       "       [1.8783374],\n",
       "       [2.276833 ],\n",
       "       [3.2799618],\n",
       "       [2.3767478],\n",
       "       [2.4595423],\n",
       "       [3.9920099],\n",
       "       [4.082672 ],\n",
       "       [2.5933764],\n",
       "       [2.6265907],\n",
       "       [2.477286 ],\n",
       "       [1.848529 ],\n",
       "       [2.5364356],\n",
       "       [2.669776 ],\n",
       "       [1.8670175],\n",
       "       [2.7371724],\n",
       "       [3.4180582],\n",
       "       [2.559124 ],\n",
       "       [2.6732533],\n",
       "       [2.9294577],\n",
       "       [3.1232483],\n",
       "       [2.4988277],\n",
       "       [2.9769347],\n",
       "       [2.3052058],\n",
       "       [3.113682 ],\n",
       "       [2.416425 ],\n",
       "       [3.516614 ],\n",
       "       [2.424477 ],\n",
       "       [3.5955715],\n",
       "       [2.255751 ],\n",
       "       [3.4893904],\n",
       "       [2.0303454],\n",
       "       [3.355632 ],\n",
       "       [2.094325 ],\n",
       "       [2.2529325],\n",
       "       [2.7514842],\n",
       "       [2.8742206],\n",
       "       [2.5938413],\n",
       "       [4.127139 ],\n",
       "       [2.8426871],\n",
       "       [3.5504088],\n",
       "       [2.3001268],\n",
       "       [2.702242 ],\n",
       "       [2.6961005],\n",
       "       [2.879571 ],\n",
       "       [2.415341 ],\n",
       "       [2.5780702],\n",
       "       [2.368145 ],\n",
       "       [3.1202586],\n",
       "       [2.6504264],\n",
       "       [3.1864913],\n",
       "       [2.5379856],\n",
       "       [2.803439 ],\n",
       "       [2.498274 ],\n",
       "       [2.668585 ],\n",
       "       [2.0109048],\n",
       "       [2.428426 ],\n",
       "       [2.6975849],\n",
       "       [3.3627405],\n",
       "       [1.9500943],\n",
       "       [2.1220791],\n",
       "       [2.8953247],\n",
       "       [2.5524914],\n",
       "       [2.842023 ],\n",
       "       [2.654913 ],\n",
       "       [2.3328195],\n",
       "       [2.7744987],\n",
       "       [1.9433093],\n",
       "       [2.4473996],\n",
       "       [2.8815506],\n",
       "       [2.3847344],\n",
       "       [3.1147006],\n",
       "       [3.1538208],\n",
       "       [2.6970422],\n",
       "       [2.435273 ],\n",
       "       [3.0736282],\n",
       "       [3.422431 ],\n",
       "       [2.717866 ],\n",
       "       [2.7879922],\n",
       "       [2.727592 ],\n",
       "       [2.2893662],\n",
       "       [2.768242 ],\n",
       "       [2.472036 ],\n",
       "       [2.7614026],\n",
       "       [3.0521638],\n",
       "       [2.3416598],\n",
       "       [2.6237006],\n",
       "       [2.5854897],\n",
       "       [2.4146187],\n",
       "       [2.6672602],\n",
       "       [2.208029 ],\n",
       "       [2.3630433],\n",
       "       [2.384017 ],\n",
       "       [3.1637385],\n",
       "       [2.5625606],\n",
       "       [2.2162294],\n",
       "       [2.3231263],\n",
       "       [1.8932674],\n",
       "       [3.3900511],\n",
       "       [2.797002 ],\n",
       "       [3.0128832],\n",
       "       [3.000744 ],\n",
       "       [2.1259685],\n",
       "       [2.291247 ],\n",
       "       [3.281098 ],\n",
       "       [2.8897808],\n",
       "       [2.2368238],\n",
       "       [2.4974384],\n",
       "       [2.8110607],\n",
       "       [2.6967278],\n",
       "       [2.431958 ],\n",
       "       [2.7172823],\n",
       "       [1.9247831],\n",
       "       [2.364863 ],\n",
       "       [3.275506 ],\n",
       "       [3.5918744],\n",
       "       [1.6645429],\n",
       "       [3.1701877],\n",
       "       [2.1609309],\n",
       "       [3.1582768],\n",
       "       [3.0741014],\n",
       "       [2.3077936],\n",
       "       [3.9861531],\n",
       "       [3.1432488],\n",
       "       [2.793617 ],\n",
       "       [2.808601 ],\n",
       "       [3.5436537],\n",
       "       [2.539282 ],\n",
       "       [2.3719234],\n",
       "       [3.0200648],\n",
       "       [2.8925793],\n",
       "       [2.3664045],\n",
       "       [2.6917064],\n",
       "       [2.5617478],\n",
       "       [2.4524488],\n",
       "       [3.2049909],\n",
       "       [2.4027333],\n",
       "       [3.2447364],\n",
       "       [2.6088119],\n",
       "       [2.2508547],\n",
       "       [2.2634623],\n",
       "       [2.9078622],\n",
       "       [1.8395064],\n",
       "       [2.6401577],\n",
       "       [3.2661374],\n",
       "       [2.0829167],\n",
       "       [2.8440895],\n",
       "       [2.9072986],\n",
       "       [3.3304253],\n",
       "       [2.8692973],\n",
       "       [2.9665651],\n",
       "       [2.7403433],\n",
       "       [2.3490052]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict+3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict +=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Headline', 'Category', 'Label']\n",
      "      ID                                           Headline     Category  \\\n",
      "0      1  Chelsea have no fit strikers ahead of trip to ...     football   \n",
      "1      2  Microsoft opens up windows to Android and Appl...  sciencetech   \n",
      "2      3  Fashion editor Meghan Blalock recounts her jou...       femail   \n",
      "3      4  Three police officers accused of stealing ?? 3...         news   \n",
      "4      5  As David Beckham shows off his beard, femail c...       femail   \n",
      "..   ...                                                ...          ...   \n",
      "222  223  Life on earth is flourishing and more diverse ...  sciencetech   \n",
      "223  224  Ed balls refuses to match Tory election pledge...         news   \n",
      "224  225  Nurse is paid ?? 2,200 for one 12-hour shift t...       health   \n",
      "225  226              How London is being sold off to Qatar         news   \n",
      "226  227  United Launch Alliance reveals plans for its r...  sciencetech   \n",
      "\n",
      "        Label  \n",
      "0    2.542996  \n",
      "1    2.743884  \n",
      "2    2.861293  \n",
      "3    3.397241  \n",
      "4    3.131291  \n",
      "..        ...  \n",
      "222  3.509242  \n",
      "223  2.428804  \n",
      "224  3.359943  \n",
      "225  2.744609  \n",
      "226  2.978937  \n",
      "\n",
      "[227 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "col_name=df2.columns.tolist()                   # 將數據框的列名全部提取出來存放在列表裏\n",
    "print(col_name)\n",
    "\n",
    "df2['Label']=y_predict   # 給city列賦值\n",
    "print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('0751231.csv',index=0,columns = ['ID','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_string = model.to_json() \n",
    "with open(\"model.config\", \"w\") as text_file:    \n",
    "    text_file.write(json_string)\n",
    "model.save_weights(\"model.weight\")\n",
    "from keras.models import load_model\n",
    "model.save('model.h5')  # creates a HDF5 file 'model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
